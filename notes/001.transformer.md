1.已有的序列转录模型多是基于RNN结构，缺点是必须从前到后的计算，即计算h_t时必须已知h_t-1，导致很难并行。

2.卷积神经网络对比较长的序列难以建模（受限于局部窗口操作，想提升感受野必须增加卷积层数）。

3.transformer是基于注意力机制，没有循环操作与卷积操作。

4.卷积操作的一个优点是可以多通道输出，每个通道其实可以看作一种模式，为了模拟此效果，transformer引入了multi-head-attention概念。

5.许多序列转录模型都是基于encoder-decoder结构的。
- 假设输入为(x1, ..., xn), xn为第n个输入的向量表示；
- encoder的输出为(z1, ..., zn),序列长度仍然为n，可理解为encoder将xn映射为另外一种向量表示zn，其中zn结合了1～n个输入的所有信息；
- decoder会将(z1, ..., zn)映射到(y1, ..., ym)，此时序列长度可能不同, n->m；
- decoder与encoder最大的不同是，解码器里的词是一个一个生成的，此过程被称为auto-regressive，比如说要得到y_t时，前面已经得到的y_1~y_t-1可作为当前的输入，这个过程就是自回归。

6.transformer架构：self-attention、point-wise, fully-connection
此处应插入transformer经典图。
