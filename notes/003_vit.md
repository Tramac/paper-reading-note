# AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE

[Paper](https://arxiv.org/pdf/2010.11929.pdf) | [Talk](https://www.bilibili.com/video/BV15P4y137jb?spm_id_from=333.999.0.0)

## Part1. 标题

- An image is worth 16x16 words: 一张图片等价于很多16x16大小的单词
- Transformers for image recognition at scale: Transformer去做大规模图像识别

## Part2. 摘要

- 当前CV中attention通常是和CNN一起使用，或者用attention替换CNN中部分操作，整体架构还是基于CNN
- 本文提出一个纯的transformer用于cv任务，不含CNN操作，效果媲美基于CNN的方法，尤其有大规模数据做预训练时

## Part3. 引言

- Transformer目前基本时NLP领域的必选模型，基本上是先用一个大的数据集做预训练，再用一个特定领域的小数据集做微调
- 一个transformer用于CV的做法基本都致力于降低序列长度，比如将CNN得到的特征（如14x14大小的特征图，序列为196）送给transformer
- 另一个transformer用于CV的做法是降低attention的计算复杂度，但是这些特定的attention操作在硬件上加速
- ViT的做法：通过把图像切分成patch，每个patch看作一个token，大大降低了序列长度，然后经过一个FC做embedding后送入transformer，transformer结构与NLP中的基本一致

**Transformer用于CV领域的一些难点**

- 如果将图片像素级拉伸为一个序列，作为transformer的输入，其中attention的复杂度为O(n^2)，其中n为序列长度（对224x224大小的图像来说=50176），计算复杂度过高

**ViT的预训练使用的是有监督训练，而NLP领域的BERT预训练时使用的是无监督（MLM和next sentence predict）**

**ViT相对于CNN的优劣**

- ViT在中型大小的数据集上（如imagenet），如果不加比较强的约束(regularization)，和同等大小的CNN模型相比会低几个点：因为transformer相对CNN来说缺少一些归纳偏置（先验知识）

> CNN中两个常说的归纳偏置：locality(局部相似性，假设是相邻的区域会有相似的特征)和translation equivariance(平移不变性，`f(g(x))=g(f(x))`，这里f可以看作卷积，g看作平移，无论是先做卷积还是先做平移，结果都是一样的)
> CNN有以上的两个归纳偏置，意味着它拥有很多的先验信息，所以它需要相对少的数据就可以学习得到比较好的模型

- ViT如果先在大的数据集上（如ImageNet-21K和JFT）进行预训练，然后在小数据集上进行微调，就会得到和同等CNN差不多或更好的结果

## Part4. 结论

- 直接用NLP领域中标准的Transformer做CV领域的问题
- ViT除了刚开始切图像patch和引入position encoding时用了一些图像特有的归纳偏置之外，再没有使用其它的归纳偏置
- 展望：分割、检测任务；DETR、ViT-FRCNN、SETR、Swin-Transformer等模型；自监督做预训练，MAE等；多模态通过Transformer大一统

## Part5. 相关工作

- Transformer在NLP领域的应用：先在大规模数据集上预训练，然后在特定数据集上做微调；BERT, GPT等
- Transformer在CV领域的应用：由于O(n^2)复杂度，在像素层面使用transformer不现实，必须做一些近似操作，比如只用图像的局部做attention、稀疏attention、轴attention等

## Part6. ViT模型

ViT在模型设计上是尽可能的按照原始的transformer来做，好处是可以把NLP领域比较成功的transformer架构直接拿来用

## Part7. 实验

## Part8. 评论
