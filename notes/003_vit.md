# AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE

[Paper](https://arxiv.org/pdf/2010.11929.pdf) | [Talk](https://www.bilibili.com/video/BV15P4y137jb?spm_id_from=333.999.0.0)

## Part1. 标题

- An image is worth 16x16 words: 一张图片等价于很多16x16大小的单词
- Transformers for image recognition at scale: Transformer去做大规模图像识别

## Part2. 摘要

- 当前CV中attention通常是和CNN一起使用，或者用attention替换CNN中部分操作，整体架构还是基于CNN
- 本文提出一个纯的transformer用于cv任务，不含CNN操作，效果媲美基于CNN的方法，尤其有大规模数据做预训练时

## Part3. 引言

- Transformer目前基本时NLP领域的必选模型，基本上是先用一个大的数据集做预训练，再用一个特定领域的小数据集做微调
- 一个transformer用于CV的做法基本都致力于降低序列长度，比如将CNN得到的特征（如14x14大小的特征图，序列为196）送给transformer
- 另一个transformer用于CV的做法是降低attention的计算复杂度，但是这些特定的attention操作在硬件上加速
- ViT的做法：通过把图像切分成patch，每个patch看作一个token，大大降低了序列长度，然后经过一个FC做embedding后送入transformer，transformer结构与NLP中的基本一致

**Transformer用于CV领域的一些难点**

- 如果将图片像素级拉伸为一个序列，作为transformer的输入，其中attention的复杂度为O(n^2)，其中n为序列长度（对224x224大小的图像来说=50176），计算复杂度过高

**ViT的预训练使用的是有监督训练，而NLP领域的BERT预训练时使用的是无监督（MLM和next sentence predict）**

**ViT相对于CNN的优劣**

- ViT在中型大小的数据集上（如imagenet），如果不加比较强的约束(regularization)，和同等大小的CNN模型相比会低几个点：因为transformer相对CNN来说缺少一些归纳偏置（先验知识）

> CNN中两个常说的归纳偏置：locality(局部相似性，假设是相邻的区域会有相似的特征)和translation equivariance(平移不变性，`f(g(x))=g(f(x))`，这里f可以看作卷积，g看作平移，无论是先做卷积还是先做平移，结果都是一样的)
> CNN有以上的两个归纳偏置，意味着它拥有很多的先验信息，所以它需要相对少的数据就可以学习得到比较好的模型

- ViT如果先在大的数据集上（如ImageNet-21K和JFT）进行预训练，然后在小数据集上进行微调，就会得到和同等CNN差不多或更好的结果

## Part4. 结论

- 直接用NLP领域中标准的Transformer做CV领域的问题
- ViT除了刚开始切图像patch和引入position encoding时用了一些图像特有的归纳偏置之外，再没有使用其它的归纳偏置
- 展望：分割、检测任务；DETR、ViT-FRCNN、SETR、Swin-Transformer等模型；自监督做预训练，MAE等；多模态通过Transformer大一统

## Part5. 相关工作

- Transformer在NLP领域的应用：先在大规模数据集上预训练，然后在特定数据集上做微调；BERT, GPT等
- Transformer在CV领域的应用：由于O(n^2)复杂度，在像素层面使用transformer不现实，必须做一些近似操作，比如只用图像的局部做attention、稀疏attention、轴attention等

## Part6. ViT模型

ViT在模型设计上是尽可能的按照原始的transformer来做，好处是可以把NLP领域比较成功的transformer架构直接拿来用

<img src="https://user-images.githubusercontent.com/22740819/146302173-3b41f378-7c80-4bb5-81b6-d8fe43013b85.png" width=600>

**ViT整体流程**：图像切分Patch -> Patch序列化 -> Patch + Position Embedding -> Transformer Encoder -> MLP Head -> Class

***1.图像切分patch***

给定一幅尺寸为(224, 224, 3)大小的RGB图像，patch_size=16:

- patch数量：N = 224^2 / 16^2 = 196
- patch尺寸：224 / 16 = 14，即(14, 14, 3)
- patch flat后维度：14 * 14 * 3 = 768
- Linear Embedding矩阵维度：D=(768, 768)

所以，图像经patch后成为一个(196, 768)大小的矩阵，然后通过Linear Embedding后得到的patch embedding维度大小是(196, 768)。至此，已经将一个vision问题转换成了NLP问题，输入为一系列的1D的token。

***2.[CLS]Token***

ViT中同样借鉴了BERT中的[CLS]特殊字符，其位置信息永远为0，根据attention机制，所有的token之间都会有信息交互，所以认为[CLS]字符的embedding有能力从其它token的embedding获取到全局的信息，从而只需要根据[CLS]的输出做最终判断即可。

所以，再加入[CLS] token之后，实际最终送入到transformer的序列长度为(197, 768)

> ViT对于[CLS] token代表全图特征还是global average pooling(GAP)后代表全局特征做了消融实验。实验显示两者的最终效果相当，但是为了和原始的transformer尽可能的保持一致，选择了添加[CLS] token来作为全图信息的代表

***3.Position Embedding如何做***

每一个patch本身有一个序号，比如1～196，然后通过一个映射表将每个序号映射为768维的向量，得到position embedding，然后将position embedding与patch embedding直接做sum操作，得到了最终送入transformer encoder的向量，维度仍然是(197, 768)。

至此，对图像的预处理操作已经处理完毕

> ViT对位置编码的方式同样做了对比实验，分别选取了1-D positional embedding、2-D positional embedding和relative positional embedding。实验结果显示，选择哪种位置编码方式都可以。解释：ViT对原图的处理是patch-level而不是pixel-level，其实对于196个patch数量不算太多，学习得到位置信息不是很难

***4.Transformer Encoder***

通过以上的数据预处理，得到了patch embedding，维度大小为(197, 768)

<img src="https://user-images.githubusercontent.com/22740819/146326041-4af0d668-478c-4b5a-89e8-49bb1daf5152.png" width=300>

- Transformer Encoder中用的也是Multi-Head Self-Attention，其中q、k、v同样是输入的patch embedding复制三份，由于是multi-head，首先需要对输入的(197, 768)降维，假设head数量H=12，则降维至(197, 768//12)=(197, 64)，然后将经过multi-head attention得到的H个(197, 64) concat起来，恢复至(197, 768)
- Attention的输出送入MLP，通常MLP中会做一次升维和降维：(197, 768) -> (197, 3072) -> (197, 768)

至此，完成了一个transformer block，其原始输入大小为(197, 768)，最终输出大小也是(197, 768)。由于输入、输出维度一致，可以有任意个block堆叠组成最终的transformer encoder

***5.归纳偏置***

相对于CNN而言，ViT缺少很多图像特有的归纳偏置，比如locality和translation equivariance。ViT中只有MLP层具有locality和translation equivariance性质的，attention肯定是全局的，添加的position embeding也是随机初始化的，没有携带2D信息，需要从头学，所以需要比较多的训练数据

***6.Hybrid Architecture***

混合网络：前面是CNN，后面是Transformer，直接拿特征图去过transformer。比如一张图经resnet50提取后，在stage4的输出大小为(14, 14)，拉直之后也是196的长度，后面过transformer的过程和之前一样

***7.不同尺寸图像上的fine-tuning***

微调时，如果使用比预训练时更大尺寸的图像，效果会更好

假如，预训练时使用的图片尺寸为224x224，微调时的图片尺寸为320x320，这时如果保持patch size不变（假设为16），那么得到的序列长度也会改变，由196变为了400。这时位置编码就会从1～196变为了1～400，这就会导致本来预训练好的位置编码失效。
> 解决方法：对原始的位置编码做二次插值直接得到新的位置编码，但是如果新序列太长可能导致掉点，这是ViT在微调时的一个局限性

## Part7. 实验

***1.ViT的三个变体***

<img src="https://user-images.githubusercontent.com/22740819/146353925-b73a3ac5-f5e9-4593-96fe-e21ab4957973.png" width=300>

***2.ViT对数据集规模的依赖***

<img src="https://user-images.githubusercontent.com/22740819/146360307-9ef640c0-6ca8-4b11-afdb-ed56a9129c20.png" width=300>

- ViT：训练快+效果好
- 小规模数据集（ImageNet-1k）上做预训练，ViT全面不如ResNet，由于缺少归纳偏置的原因；中等规模数据集(ImageNet-21k)上做预训练，ViT与ResNet持平；大规模数据集(JFT-300M)做预训练，ViT超越ResNet
- 如果数据规模小，就用CNN，如果数据规模大，就用ViT

***3.ViT、CNN、Hybrid对比***

***4.可视化分析***

***5.位置编码可视化***

- 学到了距离的信息
- 学到了行和列的距离规则

***6.ViT中的self-attention是否真的起作用

<img src="https://user-images.githubusercontent.com/22740819/146362595-66e27cba-a3af-4e6a-b209-1c5974bcfa96.png" width=300>

- self-attention在最底层刚开始就注意到全局的信息；
- CNN刚开始第一层的感受野很小，只能看到附近的信息

***7.自监督预训练***

## Part8. 评论

略
