# Masked Autoencoders Are Scalable Vision Learners

[Paper](https://arxiv.org/pdf/2111.06377.pdf) | [Talk](https://www.bilibili.com/video/BV1sq4y1q77t?spm_id_from=333.999.0.0)

## MAE与Transformer、BERT、ViT的关系

<img src="https://user-images.githubusercontent.com/22740819/146364785-7fc96742-9bc4-49ab-9f88-0d7044239cb4.png" width=300>

- Transformer: 一个纯基于attention的编码器和解码器，用于机器翻译任务
- NERT: 使用一个Transformer的编码器，拓展到更一般的NLP任务，使用了完型填空的自监督的预训练机制
- ViT: Transformer在CV领域的应用，预训练阶段是有监督的方式
- MAE: 可看作CV领域的BERT，将预训练过程扩展到无监督方式，同样通过完形填空的方式，与BERT相同

## Part1. 标题&作者

带掩码的自编码器是一个可扩展的视觉学习器

- scalable: 可扩展的，如果你的算法比较快就用efficient，如果比较大就用scalable
- autoencoder: auto并不是自动的意思，而是 自 的意思；这种说法的特点是因为你的样本和标签是同一个东西

## Part2. 摘要

- MAE是一个针对计算机视觉问题的可扩展的自监督学习方法
- 随机遮挡部分图像patch，然后恢复这些被遮挡的块
- 非对称的encoder-decoder架构
- encoder只作用在可见的patch上，被mask掉的patch不做计算，可节约计算
- decoder用于重构被mask掉的像素
- mask掉大量的patch（如75%）才有可能使自监督任务work，否则通过简单的插值就可以了，无法学习到有效的信息
- MAE只是用小规模数据集，并且使用自监督方法就可以达到很好的效果

## Part3. 关键图

<img src="https://user-images.githubusercontent.com/22740819/146370679-4a661314-422c-4bc6-bc7b-3200466f4c59.png" width=400>


## Part4. 结论

## Part5. 相关工作

## Part6. MAE模型

## Part7. 实验

## Part8. 评论
