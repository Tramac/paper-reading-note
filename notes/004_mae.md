# Masked Autoencoders Are Scalable Vision Learners

[Paper](https://arxiv.org/pdf/2111.06377.pdf) | [Talk](https://www.bilibili.com/video/BV1sq4y1q77t?spm_id_from=333.999.0.0)

## MAE与Transformer、BERT、ViT的关系

<img src="https://user-images.githubusercontent.com/22740819/146364785-7fc96742-9bc4-49ab-9f88-0d7044239cb4.png" width=300>

- Transformer: 一个纯基于attention的编码器和解码器，用于机器翻译任务
- NERT: 使用一个Transformer的编码器，拓展到更一般的NLP任务，使用了完型填空的自监督的预训练机制
- ViT: Transformer在CV领域的应用，预训练阶段是有监督的方式
- MAE: 可看作CV领域的BERT，将预训练过程扩展到无监督方式，同样通过完形填空的方式，与BERT相同

## Part1. 标题&作者

带掩码的自编码器是一个可扩展的视觉学习器

- scalable: 可扩展的，如果你的算法比较快就用efficient，如果比较大就用scalable
- autoencoder: auto并不是自动的意思，而是 自 的意思；这种说法的特点是因为你的样本和标签是同一个东西

## Part2. 摘要

- MAE是一个针对计算机视觉问题的可扩展的自监督学习方法
- 随机遮挡部分图像patch，然后恢复这些被遮挡的块
- 非对称的encoder-decoder架构
- encoder只作用在可见的patch上，被mask掉的patch不做计算，可节约计算
- decoder用于重构被mask掉的像素
- mask掉大量的patch（如75%）才有可能使自监督任务work，否则通过简单的插值就可以了，无法学习到有效的信息
- MAE只是用小规模数据集，并且使用自监督方法就可以达到很好的效果

## Part3. 关键图

***1.框架图***

<img src="https://user-images.githubusercontent.com/22740819/146370679-4a661314-422c-4bc6-bc7b-3200466f4c59.png" width=400>

- 对输入图像切分patch，然后对部分patch进行mask操作，把未被mask掉的部分取出来组成输入序列
- 把序列送入encoder中（ViT），得到每一个patch的向量表示
- 把encoder输出的序列拉长，因为需要把mask掉的patch放回原位置；没被mask掉的patch就是填上ViT后输出的特征，被mask掉的patch就只有位置信息
- 然后将新的序列送入到decoder中，然后decoder把里面的像素信息全部重构回来；target就是原始未被mask的图片
- 图中encoder比decoder画的面积大，表示主要计算量在encoder部分
- 接下游任务时只需要用到encoder，并且图片也不需要做mask，直接切patch，送入encoder得到每个patch的向量表示

***2.结果图***

<img src="https://user-images.githubusercontent.com/22740819/146479078-73a92ff5-f61d-424f-9ea3-6f49bfae717a.png" width=400>

- 有些图片mask后人都分辨不出来，MAE恢复的很好，作者可能是挑选了一些重构比较好的例子放在了论文中。

***3.mask比例对比图***

<img src="https://user-images.githubusercontent.com/22740819/146479403-dbc2cada-edd7-4e24-a003-2c51d1c9897f.png" width=400>

- mask 95%时看起来也很玄学


## Part4. 结论

- MAE在小规模数据集ImageNet-1k上，通过自监督学习的方法得到了可以媲美有监督方法的结果
- 在NLP中，一个token是一个语义单元，包含较多的语义信息；而MAE中的图像patch并不是一个语义的segment（一个patch并不一定包含一个物体或者patch是一个物体的一块）；即使如此，MAE也可以学的比较好的隐藏的语义表达
- broader impacts（如果该工作出圈可能带来的影响）：只用了图像本身的信息去学习，如果数据集中有一些bias，可能会趋向于学习数据中不好的部分；MAE是生成式模型，和GAN一样可能得到误导大家的结果；

## Part5. 导言

- CV领域的任务仍然需要百万级甚至更多的有标签数据来训练
- NLP领域的GPT、BERT等都是在无标签数据上通过自监督方式学习得到不错的效果
- CV里已有的maksed autoencoder带掩码的自编码器，如denoising autoencoder(一张图片里加入很多噪音，通过去噪来学习对这张图片的理解) 
- 最近有将BERT迁移至CV领域任务的工作，但是结果均不如在NLP领域的效果好

***What makes masked autoencoding different between vision and language？***

- 在CNN中，卷积窗口使得不好将mask放进去：在transformer中，mask是一个特定的词[MASK]，它始终是和其它词区分开的，并且一直保留到最后；而CNN中如果对图像块做mask，卷积窗口进行滑动时是无法区分这个mask个边界的（从而无法和其它未被mask的部分区分开），导致掩码部分的信息最后很难还原
- 语言和图像的信息密度不同：在NLP中，一个词就是一个语义的实体，所以对于一句话假如去掉几个词再回复会比较难；在图像中，如果简单去掉一些像素，可以很容易通过邻域的像素进行插值还原
> 所以，MAE的做法是非常高比例（75%）的随机mask掉一些块，极大降低图片的冗余性，提高任务难度，使得学习得到的模型有看到全局信息的能力，而不是只关注局部信息
- NLP中，decoder的作用是还原出被mask掉的词，词是一个高语义的东西（所以只需一个MLP即可，预测出词的标签）；而CV中，decoder的任务是还原出被mask掉的像素，它是比较低层次的表示，一个MLP是不够的

***MAE的做法***

- 随机mask掉大量的块，然后去重构被mask掉的像素信息，使用一个非对称的encoder-decoder架构
- encoder只计算未被mask掉的块，decoder计算encoder的输出以及被mask掉的信息

## Part6. 相关工作

***1.AutoEncoder在CV中的应用***

- MAE其实也是一种denoising encoder的一种形式，把一些块mask可以看作是加了很多噪声

***2.Masked image encoding***

- iGPT、BEiT

***3.自监督学习***

- contrastive learning: 主要使用的是数据增强

## Part7. MAE模型

MAE是一个简单的自编码器，输入的是未被mask的部分图像，然后重构完整的图像区域

***1.Masking***

mask操作与ViT的方式相同，先对图像切patch，然后随机均匀的采样出来一些patch，剩余的用掩码mask掉。其中关键点是mask率要高（75%），减少冗余

***2.Encoder***

MAE的encoder其实就是一个ViT，但是它只作用在未被mask掉的patch上。patch embedding + position embedding -> Transformer

***3.Decoder***

Decoder会接收两部分的信息:

- 未被mask的patch，经encoder编码之后的输出
- 被mask的patch

Decoder只在预训练阶段使用，在微调下游任务时并不需要

MAE中decoder的计算量不到encoder的1/10

***4.Reconstruction Target***

MAE的目标是重构被mask掉的图像像素

- decoder的最后一层是MLP，输出是每一个patch的向量表示；假如patch大小为(16, 16)，那么向量的维度就是256，然后再将其reshape至(16, 16)作为最终还原的像素
- 损失函数为MSE，只在被mask掉的部分使用
- 对mask掉的部分的预测输出做normalization（每个patch）

***5.Simple implementation***

- 首先把切好的patch构成一个序列，然后做patch embedding + position embedding
- 随机出25%：把序列random shuffle，只保留前25%，送入encoder
- 解码时，需要把之前mask掉的的patch加进来，此时序列长度和原始相同，然后再unshuffle操作恢复原始序列的顺序

## Part8. 实验

***1.ImageNet-1k实验***

- MAE通过自监督的方式可以从ImageNet上学习到额外的信息

***2.消融实验***

- 微调时参数全部参与训练比只调最后一层效果更好
- 送入encoder的块不包含mask部分会更好
- 预测时，如果对patch内的像素值做归一化，效果会更好
- MAE对数据增强不敏感，只做resize和随机裁剪就可以
- mask操作随机采样的方式效果更好

***3.mask ratio的影响***

<img src="https://user-images.githubusercontent.com/22740819/146488443-c586cb8e-3911-43b5-a811-a23ac45a5858.png" width=400>

mask ratio=75%最好

***4.decoder的选择***

<img src="https://user-images.githubusercontent.com/22740819/146491853-625ab38c-8b70-4a4e-9b9a-60df43dfddbb.png" width=400>

decoder depth=1时和=8时效果相当，但是速度提升明显

***5.采样策略的区别***

<img src="https://user-images.githubusercontent.com/22740819/146492057-6e216455-b366-497e-9c45-86de0bcb2a76.png" width=400>

随机采样的效果好且简单

***6.微调时层数影响***

<img src="https://user-images.githubusercontent.com/22740819/146492318-63e3e5a6-0a19-4683-adf4-82b1ec7c8560.png" width=400>

## Part9. 评论

略
