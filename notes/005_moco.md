# Momentum Contrast for Unsupervised Visual Representation Learning

[Paper](https://arxiv.org/pdf/1911.05722.pdf) | [Talk](https://www.bilibili.com/video/BV1C3411s7t9/)

## Contrast Learning基本概念

- Pretext task: 代理任务，可以是人为设定的一些规则，这些规则定义了哪些样本是相似的哪些样本是不相似的，从而可以提供一个监督信号去训练模型，也就是所谓的自监督训练
- Instance discrimination: 一种pretext task，假设从数据集中随机选取一张图片`X_i`，然后做两次数据增强得到`X_i1`、`X_i2`，因为两者来自同一张图片，语义信息不应该发生变化，就可以被当作正样本，而数据集中其它图片都可以被当作负样本。从这个例子可以看出instance discrimination的含义，个体判别，因为在这个代理任务中，每个图片自身就是一个类别
- NCE loss: 对比学习中一种常用的损失函数

## Part1. 标题&作者

使用动量的对比学习方法去做无监督的视觉表征学习

- Momentum Contrast: 动量对比学习的方法；动量在数学上可以理解为一种加权移动平均，`y_t = m * y_t-1 + (1 - m) * x_t`，可以理解为*不想让当前时刻的输出完全依赖于当前时刻的输入*，所以也上之前的输出参与其中
> 其中，m为动量的超参数，y_t-1是上一个时刻的输出，y_t是当前时刻想要改变的输出，x_t是当前时刻的输入

## Part2. 摘要

- 本文提出MoCo方法去做无监督的视觉表征学习
- 本文将对比学习看作是一个字典查询（dictionary look-up）的任务，具体来说是做了一个动态的字典，字典由一个队列queue和移动平均编码器moving-averaged encoder
- 队列里的样本不需要做梯度回传，可以放很多负样本；移动平均编码器是想让字典里的特征尽可能的保持一致
- MoCo自监督预训练可以获得比同模型有监督预训练更好的结果

## Part3. 引言

- GPT和BERT已经证明了无监督学习在NLP领域已经获得了成功，在CV领域有监督预训练还是占主导地位
> 两者的差异可能是它们所输入的信号空间不同，NLP中是离散的信号空间（比如词、词根词缀），从而可以建立一个字典空间将每个词映射成对应的特征，容易建模；CV中的输入是连续的、高维空间的信号，不像单词一样有很强的语义信息
- 目前基于对比学习的无监督方法基本都可以归纳为一种方法：动态字典dynamic dictionaries（关于动态字典的解释建议直接看视频，13:40处）
- 动态字典的方式做对比学习需要做到两点：字典尽可能大（字典越大，样本越多，含有的视觉信息越多，越能学到可区分的特征）；一致性（字典中的特征都应该用相同或相似的编码器获得，这样才能保证它们和query对比时尽可能的保持一致，否则如果特征是通过不同编码器得到的，query很有可能找到的是和它是同一个编码器得到key，而不是包含相同语义信息的key）
- MoCo与一般对比学习的框架的异同，队列queue和momentum encoder（建议直接看视频，19:15处）
- MoCo只是针对对比学习建议了一种动态字典的训练机制，可以任意选pretext task（代理任务），本文选的是instance discrimination

## Part4. 结论

- MoCo将数据集由ImageNet-1k换成Instgram-1B时，提升比较小，大规模的数据集并没有很好的被利用起来，可能需要更换代理任务解决
- MoCo是否可以采用masked auto-encoding的代理任务（MAE，大佬两年前就已经开始挖坑了）

## Part5. 相关工作

- 无监督/自监督学习一般有两个方向可以做：1.代理任务(pretext task)；2.损失函数

## Part6. MoCo方法

## Part7. 实验

## Part8. 总结
