# CLIP

[Paper](https://arxiv.org/pdf/2103.00020.pdf) | [Talk](https://www.bilibili.com/video/BV1SL4y1s7LQ)

CLIP自提出以来就引起很大关注，其方法简单但又效果很好。CLIP的迁移学习能力非常强，预训练好的模型能够在任意一个视觉分类的数据集上取得不错的效果，而且是zero-shot的，即它完全没有在这些训练集上做过训练，就能得到很好的效果。文章在30多个数据集上做了测试，包括了OCR，视频动作检测，坐标定位以及一些细分类任务，涵盖面非常广，尤其是在ImageNet上的结果，CLIP在不使用ImageNet训练集的情况下，就可以取得和之前ResNet50在有监督情况下的结果。

## Part1.标题&作者

Learning Transferable Visual Models From Natural Language Supervision：利用自然语言处理来的一些监督信号，去学习一个可以迁移效果很好的视觉模型。

Transferable: 迁移性能好

## Part2.摘要

- 目前的视觉模型训练通常是使用提前定义好的一些物体类别的集合，然后模型通过去预测这些定义好的类别，从而完成模型的训练。提前定义好的类别集合：比如ImageNet 1000类，cifar10 10类，coco 80个类，cityscapes 19个类，kinetics 400个类等。这种提前定义好的样本集合可以大大简化模型学习的难度，但是也限制了模型的泛化性
- 直接从自然语言里获取一些监督信号，只要是语言描述过的物体，就有可能让视觉模型去识别到这个物体，而不仅仅是提前定义好的那些固定的类别。
- 本文证实了用一个简单的预训练任务，就可以高效的、可扩展的去学习到最好的图像表征。具体的，给定一些图片和一些文本，模型去判断图片和哪些文本是配对的。既然要做的是配对的任务，那么所需要的训练数据就应该是配对的，本文爬取了一个4亿大小的图片、文本对，然后用自监督的方式去预训练模型。
- CLIP是利用多模态的对比学习去完成模型预训练，然后利用自然语言去引导视觉模型去做物体分类。分类也不再局限于已经学到的视觉概念，还能扩展到新的类别，也就是这个预训练模型可以直接在新的下游任务上做zero-shot推理的。

## Part3.引言

- 直接从文本数据里去预训练一个模型已经在NLP里取得了革命性的成功，比如BERT、GPT等，不论是使用自回归预测的方式还是使用掩码完形填空的方式，都是一种自监督的训练方式。所以，它的目标函数是和下游任务无关的，只是想通过预训练得到一个泛化能力好的特征，这套框架基本都是"text-to-text"，即文字进文字出，其模型架构也是跟下游任务无关的，所以在处理下游任务时，无需再针对下游任务去设计head或者其它特殊处理。比如GPT3，可以做分类、翻译、生成等。
- VirTex、ICMLM、ConVIRT均是基于transformer的工作，不同的是VirTex是用自回归预测的方式做预训练，ICMLM是利用完形填空的方式，ConvVIRT与CLIP非常相似，但是只在医疗图像上做的实验。三者在模型或数据上都没有取得很大的规模。
- 之前的一些引入自然语言信号的弱监督预训练方法效果不好，基本都是因为训练数据量级小，而CLIP收集了400M（4亿）对图像/文本对作为训练样本。模型上从ResNet50、EfficientNet到Vit-L，在大数据大模型的加持下，CLIP在多个数据集上取得很好的效果。
- 迁移学习的效果基本是和模型的大小呈正相关的，模型逐步增大，迁移学习的效果也在逐步增高，是一个比较平滑的过程。
- 作者为了进一步验证CLIP模型学到的特征的有效性，做了类似对比学习中的linear-probe验证，效果也比之前在ImageNet上有监督的结果好，而且泛化性能更好。

## Part4.模型方法

**利用自然语言信号训练视觉模型的优点**

- 不需要再去标注数据
- 
