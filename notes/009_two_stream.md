# Two-Stream Convolutional Networks for Action Recognition in Videos

[Paper](https://arxiv.org/pdf/1406.2199.pdf) | [Talk](https://www.bilibili.com/video/BV1mq4y1x7RU?spm_id_from=333.999.0.0)

视频本身是一个很好的数据来源，比2D的单个图像能够包含更多的信息，比如物体之间移动的信息、长期的时序信息、音频信号，非常适合做多模态学习。

双流网络是视频理解的一篇开山之作，它是第一个在视频领域能够让卷积神经网络可以和最好的手工特征打成平手。

## Part1.标题&作者

双流卷积神经网络用来做视频中的动作识别。

Two-Stream: 顾名思义，就是使用了两个卷积神经网络。

<img src="https://user-images.githubusercontent.com/22740819/155945646-ac96ce19-f01f-43d4-b565-9498dfa0e6cb.png" width=600> 

对于视频理解任务，一些早期的工作是把视频抽一些关键帧，分别经过一个CNN，最后将结果合并。或者将这些帧叠起来一起送给CNN，然后在网络中做一些early fusion，late fusion等，达到一种时空学习的效果。但是这些工作的效果都差强人意，甚至比不上手工设计的特征。

作者认为，之所以用一个CNN无法处理好视频问题，是因为CNN它比较擅于学习局部特征，不擅长去学习视频之中物体的移动规律。既然如此，那就事先把移动信息Motion-information提取好，也就是光流optical flow，然后让CNN直接从光流到动作学得一个映射。所以，加入一支关注Motion-information的分支(Temporal stream ConvNet)，这也就是two-stream的来源。

空间流的输入就是一张单帧图片，输出是一个分类的概率，时间流输入是一系列的光流图片，输出也是一个分类的概率，最后将两个分类概率加权平均，得到最终的预测，这就是双流网络。

**Optical Flow:** 光流是描述视频中各种物体是如何运动的。它是一个可以有效描述物体运动的特征，可以过滤背景、人物性别、穿着等等不必要的噪声，最后提取的特征完全专注于动作本身。

作者团队来自牛津大学的VGG组。

## Part2.摘要

这篇论文研究了如何使用深度卷积神经网络去做视频里的动作识别，主要的难点就是如何同时学习两种信息，一种是从静止的图像上获得appearance信息，包括形状、大小、颜色等；另一种就是物体之间的移动信息，也可以看作是视频中的时序信息。

该论文的贡献有三点：

- 提出一个双流网络：由空间流与时间流两个神经网络组成；
- 证实了即使是在少量的训练数据下，一个直接在光流数据上训练的神经网络也能取得很好的效果；
- 为了弥补训练数据上的不足，使用multi-task的学习方法，在两个数据集上去同时训练一个网络；

## Part3.引言

- 与2D图像分类任务相比，视频中的时序信息可以为识别工作提供另外一个重要线索；
- 使用视频数据的好处：视频可以提供很好的一种数据增强，因为在一个视频中，同一个物体会经历各种各样的形变、遮挡、光照改变等，这种改变多样又自然，比那些生硬的数据增强要更好；

## Part4.相关工作

- 很大程度上，视频领域的进展都是被图像领域的进展推动着走的，一般都是先有图像上的突破，然后再将方法移植到视频任务上；
- 最好的手工特征方法是用了视频前后帧点和点之间的轨道信息（dense point trajectories）；
- 之前基于神经网络的工作往往是把一系列视频帧送给网络，让模型自己去学习时空信息，使最后学得跟运动信息相关的特征，但是该方式比较难；
- DeepVideo提供的Sports-1M数据集包含100w个视频（视频帧数量超过10亿），而Kinetic和someting-someting数据集也都只有20w个视频；
- DeepVideo作者发现，如果将视频帧一张张的送给2D网络和把一系列视频帧送给3D网络或具有时空学习能力的2D网络效果是一样的，说明这种方式的时空学习并没有真的抓住物体之间的运动信息；

## Part5.双流模型

视频可以很自然的被拆分为空间部分与时间部分。空间部分就是所说的appearance信息，主要用来描述视频中的场景和物体的；时间部分主要用来描述视频中的物体是如何运动的。根据该现象，本文提出了一个双流网络框架，空间流去学习空间特征，时间流去学习运动特征，最后结果通过late fusion合并得到最终的预测。

<img src="https://user-images.githubusercontent.com/22740819/156122311-5c45c793-f302-4a14-8cbd-edcc3f9fc37e.png" width=600> 

### 5.1 Spatial stream ConvNet

- 空间流网络的输入是一张一张的视频帧，用静止图像做动作识别，其实就是图像分类任务；
- 图像中的appearace信息本身就是一个很有用的信息，因为很多动作往往都是和对应的物体联系在一起的，比如弹钢琴、打篮球等；
- 空间流网络用单帧图像作为输入的方式，就可以拿ImageNet来做预训练；
- 空间流网络基本是一个AlexNet，5层卷积+2层全连接层；

### 5.2 Temporal stream ConvNet

***Optical flow 光流是什么？***

<img src="https://user-images.githubusercontent.com/22740819/156126224-66333e77-ecfe-4682-928d-a80317b1f5e5.png" width=600> 


上图中（a）和（b）分别为前后两帧图像，（c）为光流的可视化，也就是人的动作是朝着箭头方向走的。一般的，数学上表示光流时会将其拆成两个方向，即水平方向上的位移和竖直方向上的位移，可视化结果分别为图（d）和（e）。

***光流如何表示？***

具体地，假如原始输入的图像维度为(240, 340, 3)，然后经过光流预测算法（无论什么光流预测算法都可以），这两张图像就会得到一张光流图像，而光流图的维度为(240, 340, 2)，维度2其实就代表水平和竖直上的两个维度结果，对应于图中(d)和(e)，图中(d)(e)两张图的维度其实就是(240, 340, 1)。以下两点需要说明：

- 图像的输入和最后预测得到的光流大小是一样的，因为每个像素点其实都有可能运动，如果不运动那它的运动幅度就是0，总之，每个像素点都会有对应的光流值，这也就是dense optical flow（密集光流）的来源；
- 光流计算时，每两张图会计算得到一张光流，如果视频长度为L帧，最后算得的光流就是L-1帧；

***如何使用光流？***

时间流网络的输入是由多个光流图像叠加在一起的，叠加的方式有以下两种：

<img src="https://user-images.githubusercontent.com/22740819/156129300-8b5f4ab7-de2c-443d-8b31-57bc212f6eef.png" width=600> 

**Optical flow stacking:** 直接把多帧光流图像叠加在一起，如上图左侧。该方式简单，不用做预处理、后处理，直接将光流图stack即可，但缺点是没有充分的利用光流的信息；

**Trajectory stacking:** 根据光流的轨迹，在轨迹上进行光流数值的叠加。如上图右侧，当你知道上一张图中的p1点，在下一张图时已经移到了p2时，那么我们在下一张光流图里就从p2点开始去找它在它下一帧里所对应的位置，这种方式更加合理，很好的利用了光流信息，可惜的是，实验显示左边直接叠加的方式效果更好。

***Bi-directional optical flow双向光流***

Bi-directional是一个非常常见的技巧，不仅是在光流里，在视觉其它领域甚至NLP领域都有使用。具体的，上述两种光流连接方式都是单向的，而反过来算应该也是合理的，比如一个篮球从a点到b点和从b点到a点都是可以的。

为了公平对比，对于L+1帧图像，前半段用前向光流，后半段用反向光流，最终得到L帧光流图像，大小为(h, w, 2L)，以此来作为时间流网络的输入。

- 时间流网络结构和空间流网络结果基本一致，只是第一层卷积层的输入维度不同，空间流为3，而时间流为2L；
- 光流图像叠加时，具体的是先进行水平方向的叠加，再进行竖直方向的叠加，即(x1, ..., xl, y1, ..., yl)；

### 5.3 实现细节

***如何做测试？***

测试时，无论视频有多长，就从视频里等间距的去抽取25帧，帧数是固定的。因为论文中所用的数据集是UCF101和HMDB51，视频长度都是在5～7秒之间，假如帧率为30，总共就150～210帧左右，如果取25帧也就是每隔78帧左右取一帧，也就是每1/3秒取一帧，帧间变化其实不是很大，然后对每一帧取四个角和中间区域作crop，再翻转同样操作，这样一张图就变成了10张图，那么对于一个视频25帧图像来说，最终就变成了250个crop，然后每张图都会通过2D的空间流网络得到一个结果，最后这250帧的结果取加权平均得到最终预测结果。光流结果也是相同的操作。

***如何预处理光流以及如何计算光流？***

光流提取耗时：目前OpenCV中成熟的抽取光流的方法，对于一对图像在GPU上的计算耗时是0.06s。对于UCF101数据集，则需要10000x10x30x0.06s，要一百多个小时。对于100w规模的数据集，耗时将会更长，假如用8卡GPU，光抽取光流都要一个月时间。

光流存储成本：每一点都是有光流值的，也就是密集的表示，如果将结果存下来需要很大的空间。以UCF101数据集来说，存下所有的光流需要1.5T的空间，即使有空间也会受限于io速度。

基于以上问题，论文中将光流值rescale到1～255，然后存成图片，每个光流图也就十几kb，存储空间由1.5T降到了27G。

## Part6.实验

<img src="https://user-images.githubusercontent.com/22740819/156139054-1430c18c-4702-427e-8d29-c1a4412d0689.png" width=600> 

左图说明了空间流网络使用预训练模型的必要性。右图说明时间流中光流叠加方式的区别，以及双向光流的效果。

<img src="https://user-images.githubusercontent.com/22740819/156139581-979c14ed-1f51-4391-b205-f7b3e1415938.png" width=600> 

two-stream与其它方式的比较。

## Part7.结论

- 本文提出了two-stream方法，一个空间流一个时间流，实验证明只用时间流分支效果也很好；
- 虽然用了光流，但模型也不复杂，没用3D网络，也没用LSTM；
- 未来工作：时间流网络利用预训练模型；为什么基于轨迹的光流叠加方式效果差；camera motion对光流计算的影响。
