# Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset

[Paper](https://arxiv.org/pdf/1705.07750.pdf) | [Talk](https://www.bilibili.com/video/BV1tY4y1p7hq?spm_id_from=444.41.list.card_archive.click)

两个重要贡献：提出一个新的模型I3D和Kinetics数据集。

## Part1.标题&作者

Inflated：扩大、膨胀，也就是说如何把一个2D网络扩张到3D模型，而无需专门的去设计一个视频理解网络，可以使用2D里已经设计好的网络，比如VGG、ResNet等，直接将其扩张到3D即可。

Quo Vadis：一部电影名。

## Part2.摘要

当前的视频数据集都太小，比如UCF-101（101类，13000个视频）和HMDB-51（51类，7000多个视频），不利于CNN的发挥，无法有效体现出各个方法的优劣。所以本文新提出了一个Kinetics数据集，其中有400个类，每一类都有超过400个视频段落，每个段落为10s时长，均是把人体的动作从视频中精准的抠出来的，是一个标注非常好的数据集。

同时，本文提出一个双流的Inflated 3D网络，其来自于2D网络的扩张：拿一个已经调好的网络，比如ResNet，把其中所有的3x3的卷积核或者pooling操作，全部变成3x3x3，以此作为视频理解模型。其好处是，不用专门去设计一个针对视频理解的网络了。这种inflation的方式一直到现在都有在用，比如Timesformer就是把Vision Transformer给inflate。

在Kinetics数据集上预训练之后，I3D网络在HMDB-51和UCF-101数据集上分别能达到80.9%和98.0%的效果。

## Part3.引言

在图像任务中，ImageNet的出现提供了一种用大规模数据集预训练然后迁移到小规模数据集上finetune的范式。然而在视频领域还没有这样的大规模的用于预训练的数据集，所以本文提出了Kinetics数据集，其有400个类别，每个类别有超过400个样本，并且每一个样本都来自于一个独一无德youtube视频，其多样性是非常好的。而UCF-101数据集，其中很多小的clip都是从同一个长视频中抽取出来的，多样性较差。

然后本文对比了三种主流的视频理解模型在该数据集上的表现：

- CNN + LSTM
- 3D CNN
- 双流神经网络

以上三种方式主要区别就是如何利用视频中的时序信息的。所以说LSTM，3D网络，双流中的光流，分别代表了三种使用时序信息的流派。

通过在大规模数据集上预训练，然后在小规模数据集上微调的方式，发现以上三种方式的网络表现参差不齐，提升也都不是很显著。所以本文结合各种方式的优点，提出了Two-Stream I3D，其中I3D本身就是把2D网络中的2D kernel扩展为3D，包括卷积和Pooling层。用双流是因为即使使用了3D网络，针对于局部的运动信息学的还是不够好，所以还需加入光流信息才能达到比之前方法好的效果。

## Part4.相关工作和I3D模型

在图像领域，已经有了一些列的主导模型，比如VGG，ResNet等，然而在视频领域的模型一直也没有一个定论，其实
