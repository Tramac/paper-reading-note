# Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset

[Paper](https://arxiv.org/pdf/1705.07750.pdf) | [Talk](https://www.bilibili.com/video/BV1tY4y1p7hq?spm_id_from=444.41.list.card_archive.click)

两个重要贡献：提出一个新的模型I3D和Kinetics数据集。

## Part1.标题&作者

Inflated：扩大、膨胀，也就是说如何把一个2D网络扩张到3D模型，而无需专门的去设计一个视频理解网络，可以使用2D里已经设计好的网络，比如VGG、ResNet等，直接将其扩张到3D即可。

Quo Vadis：一部电影名。

## Part2.摘要

当前的视频数据集都太小，比如UCF-101（101类，13000个视频）和HMDB-51（51类，7000多个视频），不利于CNN的发挥，无法有效体现出各个方法的优劣。所以本文新提出了一个Kinetics数据集，其中有400个类，每一类都有超过400个视频段落，每个段落为10s时长，均是把人体的动作从视频中精准的抠出来的，是一个标注非常好的数据集。

同时，本文提出一个双流的Inflated 3D网络，其来自于2D网络的扩张：拿一个已经调好的网络，比如ResNet，把其中所有的3x3的卷积核或者pooling操作，全部变成3x3x3，以此作为视频理解模型。其好处是，不用专门去设计一个针对视频理解的网络了。这种inflation的方式一直到现在都有在用，比如Timesformer就是把Vision Transformer给inflate。

在Kinetics数据集上预训练之后，I3D网络在HMDB-51和UCF-101数据集上分别能达到80.9%和98.0%的效果。

## Part3.引言

在图像任务中，ImageNet的出现提供了一种用大规模数据集预训练然后迁移到小规模数据集上finetune的范式。然而在视频领域还没有这样的大规模的用于预训练的数据集，所以本文提出了Kinetics数据集，其有400个类别，每个类别有超过400个样本，并且每一个样本都来自于一个独一无德youtube视频，其多样性是非常好的。而UCF-101数据集，其中很多小的clip都是从同一个长视频中抽取出来的，多样性较差。

然后本文对比了三种主流的视频理解模型在该数据集上的表现：

- CNN + LSTM
- 3D CNN
- 双流神经网络

以上三种方式主要区别就是如何利用视频中的时序信息的。所以说LSTM，3D网络，双流中的光流，分别代表了三种使用时序信息的流派。

通过在大规模数据集上预训练，然后在小规模数据集上微调的方式，发现以上三种方式的网络表现参差不齐，提升也都不是很显著。所以本文结合各种方式的优点，提出了Two-Stream I3D，其中I3D本身就是把2D网络中的2D kernel扩展为3D，包括卷积和Pooling层。用双流是因为即使使用了3D网络，针对于局部的运动信息学的还是不够好，所以还需加入光流信息才能达到比之前方法好的效果。

## Part4.相关工作和I3D模型

在图像领域，已经有了一些列的主导模型，比如VGG，ResNet等，然而在视频领域的模型一直也没有一个定论，其实直到现在也没有一个定论是到底用2D还是3D甚至是Transformer来作为视频理解模型。

当时来看，利用时序信息的方式主要有三种：1）LSTM；2）3D模型；3）光流。本文提出了一种TwoStream Inflated 3D ConvNets。

### 为什么使用Inflated

> 因为之前的3D网络的参数量过于巨大，但是又没有合适的、足够的视频数据去预训练，就导致3D网络不能太深，比如ICCV15的工作C3D，其深度只有8层，并且效果也没有超过当时的双流网络。而本文的方法在使用了inflate操作之后，就可以直接使用一些比较深的比如VGG，ResNet等一些效果比较好的网络，而且在使用了这些2D网络的预训练参数作为初始化之后，I3D网络也不需要很多的视频数据去训练了。

### 为什么使用光流

> 其实就是发现用光流比不用效果好。

本文所使用的网络结构是从Inception-v1经过inflated得来的，之所以不使用ResNet是因为当时有很多论文做过消融实验，发现在视频任务上Inception结构比ResNet效果稍微好一些。但是由于ResNet太过统治地位，所以在一年后的Non Local论文里作者又用ResNet把I3D实现了。

下面通过图2来对比说明各种模型结构的异同：

<img src="https://user-images.githubusercontent.com/22740819/161266552-a1c80409-72b5-4426-8a9f-b61a2d92d631.png" width=800>

a）CNN + LSTM：此方式更多的是把视频看做一种图像分类问题，从image1到imageK其是一帧帧图像去过神经网络模型的，整个抽特征的过程是完全分开的，抽完特征之后再送入一个lstm网络，lstm是可以进行时序建模的，所以可以把每个时间戳的特征糅合起来，得到视频特征，经过一系列计算，用最后一个时间戳的特征经过一个全连接层得到分类结果。但是这种方法在一些数据集上表现并不是很好。

b）3D-ConvNet：该方式比较暴力，就是将一个视频分成多个视频片段，每个视频片段中包括K张图片，然后将这些图片当作一个volume整个送给网络。这也就意味着你的网络可以进行时空学习，就是说网络的卷积核必须要是3维的了，不仅要处理二维上的图像，而且还要处理额外的时间维度，卷积核尺寸也就是3x3x3，这就会导致模型的参数量很大，相当于所有层都多了一个维度。这种方式由于参数量大，在小的数据集上效果可能不是很好，但是在大数据集上可以显现出优势。

c）Two-Stream：
