# Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset

[Paper](https://arxiv.org/pdf/1705.07750.pdf) | [Talk](https://www.bilibili.com/video/BV1tY4y1p7hq?spm_id_from=444.41.list.card_archive.click)

两个重要贡献：提出一个新的模型I3D和Kinetics数据集。

## Part1.标题&作者

Inflated：扩大、膨胀，也就是说如何把一个2D网络扩张到3D模型，而无需专门的去设计一个视频理解网络，可以使用2D里已经设计好的网络，比如VGG、ResNet等，直接将其扩张到3D即可。

Quo Vadis：一部电影名。

## Part2.摘要

当前的视频数据集都太小，比如UCF-101（101类，13000个视频）和HMDB-51（51类，7000多个视频），不利于CNN的发挥，无法有效体现出各个方法的优劣。所以本文新提出了一个Kinetics数据集，其中有400个类，每一类都有超过400个视频段落，每个段落为10s时长，均是把人体的动作从视频中精准的抠出来的，是一个标注非常好的数据集。

同时，本文提出一个双流的Inflated 3D网络，其来自于2D网络的扩张：拿一个已经调好的网络，比如ResNet，把其中所有的3x3的卷积核或者pooling操作，全部变成3x3x3，以此作为视频理解模型。其好处是，不用专门去设计一个针对视频理解的网络了。这种inflation的方式一直到现在都有在用，比如Timesformer就是把Vision Transformer给inflate。

在Kinetics数据集上预训练之后，I3D网络在HMDB-51和UCF-101数据集上分别能达到80.9%和98.0%的效果。

## Part3.引言

在图像任务中，ImageNet的出现提供了一种用大规模数据集预训练然后迁移到小规模数据集上finetune的范式。然而在视频领域还没有这样的大规模的用于预训练的数据集，所以本文提出了Kinetics数据集，其有400个类别，每个类别有超过400个样本，并且每一个样本都来自于一个独一无德youtube视频，其多样性是非常好的。而UCF-101数据集，其中很多小的clip都是从同一个长视频中抽取出来的，多样性较差。

然后本文对比了三种主流的视频理解模型在该数据集上的表现：

- CNN + LSTM
- 3D CNN
- 双流神经网络

以上三种方式主要区别就是如何利用视频中的时序信息的。所以说LSTM，3D网络，双流中的光流，分别代表了三种使用时序信息的流派。

通过在大规模数据集上预训练，然后在小规模数据集上微调的方式，发现以上三种方式的网络表现参差不齐，提升也都不是很显著。所以本文结合各种方式的优点，提出了Two-Stream I3D，其中I3D本身就是把2D网络中的2D kernel扩展为3D，包括卷积和Pooling层。用双流是因为即使使用了3D网络，针对于局部的运动信息学的还是不够好，所以还需加入光流信息才能达到比之前方法好的效果。

## Part4.相关工作和I3D模型

在图像领域，已经有了一些列的主导模型，比如VGG，ResNet等，然而在视频领域的模型一直也没有一个定论，其实直到现在也没有一个定论是到底用2D还是3D甚至是Transformer来作为视频理解模型。

当时来看，利用时序信息的方式主要有三种：1）LSTM；2）3D模型；3）光流。本文提出了一种TwoStream Inflated 3D ConvNets。

### 为什么使用Inflated

> 因为之前的3D网络的参数量过于巨大，但是又没有合适的、足够的视频数据去预训练，就导致3D网络不能太深，比如ICCV15的工作C3D，其深度只有8层，并且效果也没有超过当时的双流网络。而本文的方法在使用了inflate操作之后，就可以直接使用一些比较深的比如VGG，ResNet等一些效果比较好的网络，而且在使用了这些2D网络的预训练参数作为初始化之后，I3D网络也不需要很多的视频数据去训练了。

### 为什么使用光流

> 其实就是发现用光流比不用效果好。

本文所使用的网络结构是从Inception-v1经过inflated得来的，之所以不使用ResNet是因为当时有很多论文做过消融实验，发现在视频任务上Inception结构比ResNet效果稍微好一些。但是由于ResNet太过统治地位，所以在一年后的Non Local论文里作者又用ResNet把I3D实现了。

### 主流视频模型的异同

下面通过图2来对比说明各种模型结构的异同：

<img src="https://user-images.githubusercontent.com/22740819/161266552-a1c80409-72b5-4426-8a9f-b61a2d92d631.png" width=800>

a）CNN + LSTM：此方式更多的是把视频看做一种图像分类问题，从image1到imageK其是一帧帧图像去过神经网络模型的，整个抽特征的过程是完全分开的，抽完特征之后再送入一个lstm网络，lstm是可以进行时序建模的，所以可以把每个时间戳的特征糅合起来，得到视频特征，经过一系列计算，用最后一个时间戳的特征经过一个全连接层得到分类结果。但是这种方法在一些数据集上表现并不是很好。

b）3D-ConvNet：该方式比较暴力，就是将一个视频分成多个视频片段，每个视频片段中包括K张图片，然后将这些图片当作一个volume整个送给网络。这也就意味着你的网络可以进行时空学习，就是说网络的卷积核必须要是3维的了，不仅要处理二维上的图像，而且还要处理额外的时间维度，卷积核尺寸也就是3x3x3，这就会导致模型的参数量很大，相当于所有层都多了一个维度。这种方式由于参数量大，在小的数据集上效果可能不是很好，但是在大数据集上可以显现出优势。

c）Two-Stream：结合光流与图像，光流自身蕴涵了非常准确的的物体运动信息，变相的是一种视频里时序信息的一个特征表示。双流网络本身比较简单，而且对模型的要求也比较低，训练简单，而且效果也好。双流包含两个2D网络，即空间流与时间流，其中空间流的输入为一帧或多帧图像，其主要负责学习场景信息；时间流的输入为光流图像，用来负责学习运动信息，两个分支分别得到分类特征或者结果，然后做一些late fusion操作（即在logit层做融合，也就是将分类结果融合）。

d）3D-Fused Two-Stream：它可以看作是b)和c)两种方式的结合，开始阶段是按照双流网络的方式来做，用两个2D CNN分别提取图像与光流的特征，但是在两个特征融合时并不是采用简单的加权平均，而是用一个比较小的3D网络来替代，如果说之前的融合方式为later fusion，那么这种融合方式就是early fusion（特征层面先融合，然后再去做分类）。

e）Two-Stream 3D-ConvNet：也就是本文提出的双流i3d网络，作者认为在有足够多的训练数据下，3D CNN的效果是要好于2D CNN的，但是3D CNN对于时序信息的获取并没有使用光流好，所以本文还是保留了光流，但是双流中每一支都是一个3D CNN，所以也就不存在early fusion和late fusion了，也就没有必要在提取特征之后再加一个3D CNN来做融合，所以最终直接做了加权平均。

### I3D的实现细节

#### 1.如何做inflate

简单来说，就是将一个2D网络暴力的变成3D网络。以ResNet50为例，只要遇到一个2D的卷积核，就将其变为3D的卷积核，遇到2D的pooling层，就将其变为3D的pooling层，而整体的架构都不变。这就是inlfate过程，其优点就是不用再去设计网络结构，所有设计好的2D网络都可以直接拿来用。

#### 2.如何做bootstrap

上面通过inflate操作得到一个视频理解网络只是第一步，更难的问题是如何把这个3D网络训练起来。

Bootstrapping的字面意思是引导，也就是当你已经有一些东西之后，然后在其上面做一些改进从而让这个东西变得更好。本文的意思就是你如何从一个已经训练好的2D网络（如resnet50）出发，用它去初始化一个3D模型，然后在这个初始化好的3D模型上面继续去做训练。

一般的，如果想用一个已经预训练好的模型来做初始化，最简单有效的方式是两个网络应该是一模一样的，这样就可以把预训练好的参数搬过来就可以了。但现在是预训练好的是一个2D网络，而即将要训练最后要用的网络是一个3D网络，虽然它们整体架构相同，但是具体到每一步操作它都是不一样的，那么如何用2D模型参数去初始化3D模型呢？

对于2D网络的预训练模型初始化，对于同一张图像，在预训练模型上的输出与参数初始化后的模型输出，理论上是相同的（模型一致，参数相同）。受此启发，假如对同一张图片反复地做N次复制粘贴，最终得到一个视频（视频中全都是同样的视频帧），如果有某种方式将2D预训练模型的参数对3D网络作初始化，使得两者在该视频上的输出是一致的，那么就能说明该初始化方式是有效的。**具体的做法就是将所有的2D filter全都在时间的维度上也复制粘贴N次，和视频对应起来，那么3D网络的参数就成了WxN，假如2D网络的输入为WxX，那么3D网络就是得到了N份的WxX，如果想让两者的输出保持一致，就需要做一些scaling操作，也就是在所有的2D filter上除以N，最终N倍的WxX除以N之后仍然是WxX**。这样就能保证你的输入不管是一张图还是一个视频，通过网络的输出都是一样的。

#### 3.如何控制池化层让感受野在合适的范围

#### 4.如何将一个3D网络变为双流的3D网络
