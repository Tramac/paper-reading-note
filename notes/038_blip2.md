# BLIP-2

[paper](https://arxiv.org/abs/2301.12597) | [github](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)

## TL;DR

- 提出 Q-Former
- 两阶段只训练 Q-Former 部分，vision encoder 和 LLM 始终不参与训练
- 多任务训练：ITC、ITG、ITM

## Abstract

大模型的预训练成本越来越高，本文提出的 BLIP-2，是一种通用且高效的预训练策略，可以从已有的预训练 imge encoder 和 LLM 引导 vision-language 预训练。BLIP-2 的核心是利用一个轻量的 Query Transformer，经过两阶段的训练，实现 vision encoder 与 LLM 的特征空间对齐。

## 1 Introduction

在训练图文多模态模型时，现有方法往往采用端到端重新预训练的模式，这会产生两个比较突出的问题：

- **训练成本高**：多模态模型需要大量数据，且参数量大，端到端 train from scratch 的成本比较高；
- **模型灾难性遗忘**：由于数据分布很可能不一致，如果直接将单模态的预训练模型加入到多模态模型中进行联合训练，可能会产生灾难性遗忘问题；

多模态大模型的核心问题是 **cross-model alignment**，即模态对齐。本文提出 BLIP-2：

- **利用 pre-trained 单模态模型**，避免train-from-scratch，减少训练成本；
- **将单模态模型的参数进行冻结**，从而避免灾难性遗忘问题，充分利用已经训练好的单模态模型；
- **利用Q-Former来对图文进行对齐**，从而让图像和文本产生交互；

## 2 Related Work

**End-to-end Vision-Language Pre-training**

SimVLM、PaLI、CoCa、OPT

**Modular Vision-Language Pre-training**

VisualGPT、Flamingo

## 3 Method

BLIP-2 的训练分为两个阶段：1）vision-language representation learning stage with a frozen image encoder；2）vision-to-language generative learning stage with a frozen LLM。

### 3.1 Model Architecture

<center>
    <img src="https://github.com/user-attachments/assets/8d78cbf7-9557-402e-a650-07801d94bc48">
</center>

模型结构主要由三部分构成：**Image encoder、Q-Former、LLMs**。

Q-Former 包括两个 transformer submodule（左侧的为 image transformer，右侧为 text transformer），它们共享一个 self-attention 层。随机初始化一个 learned queries 作为 image transformer 的输入，Image Encoder 提取到的视觉特征输入至 cross-attention 与 query 融合；文本输入 text transformer 提取得到文本特征。Learned Queries 同时还可以通过共享参数的 self-attention 与 text 特征进行融合。

> Q-Former 中的 transformer 结构由 BERT-base 的权重做初始化，image transformer 中的 cross-attention 部分随机初始化，参数量共 188M。

> Learned Queries 的 size 为 (32, 768)，相比于 image encoder 输出的图像特征 (257, 1024) 轻量很多。

### 3.2 第一阶段：representation learning stage

<center>
    <img src="https://github.com/user-attachments/assets/a29e4bf0-3d6d-4589-8c37-b79cd6ed1eba">
</center>

在该阶段的训练使用了三个任务：

- **Image-Text Contrastive Learning（ITC）**：通过对比学习对齐图像表示和文本表示，使其互信息最大化，使用的 loss 是经典的 InfoNCE loss；

  > image_feats是多个特征，因为query tokens是32个，而text_feat是单个特征，即 [CLS] token 的特征。因此在计算图文相似度的时候，会将 32 个特征与图像特征以此计算相似度，选择最大值作为最终相似度。
  >
  > 在该阶段为了避免信息泄露，使用了一个单模态的 self-attention mask，所以 queries 和 text 在该阶段是互相看不到的。
  >
  > 因为 image encoder 是 freeze 的，所以可以使用更大的 batch size，不需要像 BLIP 中那样维护一个队列来放足够多的负样本。

- **Image-grounded Text Generation (ITG)**：该任务是给定 image 作为 condition，令 Q-Former 生成文本；

  > 因为 Q-Former 的结构不允许图像特征与文本特征直接交互，图像信息只能先经过 queries 提取，再通过 self-attention 与 text 融合，所以该阶段使用一个 causal self-attention mask 来控制 query-text 的交互，queries 的 token 可以相互看到，但是它看不到 text token，而 text token 可以看到所有的 queries token 以及自身之前的 token。

- **Image-Text Matching (ITM)**：旨在学习图像和文本特征之间的细粒度对齐。这是一个二分类任务，预测输入的 image-text 对是 positive 还是 negative 的。

  > 在该阶段使用一个 bi-directional self-attention mask，即 queries 和 text tokens 全部可以相互看到。

在该阶段 image encoder 冻结，使用 image-text pair 数据仅训练 Queries 和 Q-Former 部分，让Queries能够将 Image Encoder 中提取的原始图像特征，转化为和文本特征很接近的特征，这个阶段相当于在将图像特征拉近到文本特征。

### 3.3 第二阶段：generative pre-training stage

<center>
    <img src="https://github.com/user-attachments/assets/299f8559-c298-478e-a5c6-8af23a18e916">
</center>

该阶段通过一个 FC 层将 Q-Former 与 frozen LLM 连接起来。整个流程是：frozen Image Encoder 生成原始的图像特征，而 query tokens 和 Q-Former 从原始图像特征中提取关键的图像特征 query embeddings，然后该特征经过全连接层映射到 LLMs 的文本 embedding 空间中。在这里，query embeddings  就相当于soft visual prompts，和文本embedding一起，输入到冻结的LLMs中，最后生成目标文本。

BLIP-2 尝试了两种 LLM 结构，即 decoder-based LLMs 和 encoder-decoder-based LLMs。对于 encoder-decoder-based 来说，text 会被分为两部分，第一部分会作为 encoder 的输入，第二部分作为 decoder 的监督信号。

### 3.4 Model Pre-training

**Pre-training data**

训练数据使用的和 BLIP 中的一致，共 129M 张图，包括 COCO、Visual Genome、CC3M、CC12M、SBU、LAION400M 等；然后每张图像先用 CapFilt method 生成一条 caption，再用 BLIP-large 生成 10 条 caption，然后将这些 caption 用 CLIP ViT-L/14 计算图文相似度之后排序，留取得分最高的两个 caption 作为训练数据，每次从两个 caption 中随机选取一个作为 image-text pair 训练。

**Pre-trained image encoder and LLM**

image encoder 使用的是 CLIP ViT-L/14 或 EVA-CLIP ViT-g/14，选择倒数第二层的特征作为 image features；对于 decoder-based LLMs 用的是 OPT，encoder-decoder-based LLMs 用的是 FlanT5。

**Pre-training settings**

略。

## 4 Experiment

这里只放一下和 SOTA 方法的效果对比：

<center>
    <img src="https://github.com/user-attachments/assets/ab18a625-0746-4a66-80bc-2b79b6781b38">
</center>

上表可以看出BLIP-2在利用更少可训练参数的基础上，在VQA、Image Captioning、ITR等多模态任务上都取得不错的效果。
