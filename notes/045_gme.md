# GME: Improving Universal Multimodal Retrieval by Multimodal LLMs

[paper](https://arxiv.org/pdf/2412.16855) | [code](https://github.com/modelscope/ms-swift/blob/main/examples/train/embedding/train_gme.sh)

## HighLight

- 代码开源
- 通过两阶段训练挖掘难负样本
- 仅训练了英文，不支持中文
- 仅训了 LoRA，全参数训练效果不如 LoRA

## 摘要

- 提出一种训练数据合成 pipeline

- 提出一个通用的多模态 ebdedding 框架：GME

- 提供了一个多模态检索 benchmark：UMRB

## 引言

多模态检索之前的做法：

- 基于CLIP架构，设计不同模态间的融合方式

- 利用 visual plugin 模块，将 vision token 送入 LLM，进而得到多模态特征

本文有一个亮点是**支持visual documents**，比如文档截屏。

## Universal Multimodal Retrieval

<img width="576" alt="image" src="https://github.com/user-attachments/assets/274a8662-899a-4b49-852b-f7646c420a81" />

- Single-Modal Retrieval：图搜图、文搜文
- Cross-Modal Retrieval：文搜图
- Fused-Modal Retrieval：query 和 candinates 可以是任意组合

### UMRB

UMRB 包括 47 个评估集，主要由一些公开数据集的子集组成。

<img width="562" alt="image" src="https://github.com/user-attachments/assets/56fc1947-fcaf-4719-b72b-1d35f03b2ab2" />

鉴于 UMRB 的规模庞大，为了加快实验验证和分析，我们从每个类别中抽取了一个数据集子集，构成一个较小的数据集，名为 UMRB-Partial，占比 39%。

## 方法

<img width="568" alt="image" src="https://github.com/user-attachments/assets/4f3cda11-117b-433f-ab6e-eca1e4348d4a" />

- 输入可以是 image、text、image-text pairs
- 最后一层的最后一个 token 的 hidden state 作为 embedding
- 训练时不训原来的目标，即没有 AR 任务
- 训练基于对比学习

**难负样本**

对于难负样本的挖掘，使用了两阶段的训练：

1. Initial Training：先使用随机挑选的负样本训练得到模型 $$M_1$$
2. Hard Negative Mining and Continue Training：对每个 query 使用 $$M_1$$ 检索 top K 个 candidates，挑选出不相关的样本作为难负样本（需要标注），使用这些难负样本继续训练。

### Fused-Modal 数据合成

<img width="582" alt="image" src="https://github.com/user-attachments/assets/005c43dc-0b17-4b57-acae-fa09ed1a3bb5" />

数据合成的整体思路是根据 candidate 生成 query，然后组成 query-candidate 对。

首先需要获取高质量的 candidates，我们主要从维基百科的段落中提取数据，为了增加 candidates 的 domain 多样性，使用一个 domain classification 模型，将提取到的数据分类，只保留分类得到大于0.5的数据，然后按类别均匀抽样，最终得到 313,284 的 candidates，每个都包括图像和文本。

**Doc2Query Generation**

将每个 candidates 的文本先送入 Qwen2.5-72B-Instruct 生成 query，为了保证生成的 query 的质量，使用 gte-Qwen2-1.5B-instruct 将 candidates 的文本建了一个索引库，然后用生成的 query 检索这个库，如果对应的 candidates 文本出现在 top20 中，则认为是高质量 query，其实就是为了保证 candidates 的 text 与 生成的 query 的相关性。

这一步之后，首先可以得到 T --> IT 训练数据。

**Entity Extraction and Query Rewrite**

为了将 query 扩充为 image-text pair，先用一个 LLM 提取出 query 文本中的实体，同时对 query 文本进行冲泻，然后检索到实体的图像，从而构成 image-text query。

举个例子，假如第一步生成的 query 文本为：“Where is Iris pseudacorus native?”。利用 LLM 将其重写为 “Where is the native habitat of this plant?”，并且提取其中的实体 “Iris pseudacorus”，然后找到一张 Iris pseudacorus 图片，最终构成 image-text 对。

经过这一步之后，可以得到 IT --> IT 训练数据。

**Image Retrieval and Generation**

对于上一步中图像的获取，采用了两种方式。一是用 Google 直接搜索，选取 top5 的结果；二是使用文生图模型 FLUX.1-dev 直接生成。

对于第二种方式，先用一个 LLM 将实体概念进行扩充后在进行文生图。

**Data Filtering**

为了保证合成数据的高质量，使用 Promptagator 对数据进行了过滤。

我们发现对于 FLUX.1 生成的数据质量通常比较高，但是 Google 检索得到的图经常有噪声。所以，对于 Google 检索得到的图，用 CLIP 计算图文的相关性，丢弃到得到小于 0.2 的数据。

经过上面的 pipeline，共生成了 1,102,000 条数据。

## 实验

训练数据共800W，其中文-文100W，图-图100W，文-图200W，fused 数据 200W，另外200W不知道是什么，paper 中没说清楚。

Backbone 使用的是 QWen2-VL 的 2B 和 7B，并使用 LoRA 训练。

<img width="1167" alt="image" src="https://github.com/user-attachments/assets/11824228-918e-40c7-b239-b9cd394d67b2" />

**GME 特征是否足够通用？**

具体来说，embedding 是否具有模态通用性，即不同模态中相同语义内容的 embedding 空间距离是否相近。同时，在每个模态的子空间中，特征也足够具有分辨力。

<img width="562" alt="image" src="https://github.com/user-attachments/assets/372e160b-d06b-4c73-901d-6f16d9ff91e9" />

- CLIP 对不同模态之间的语义相似性没有判别能力
- VISTA 对特征的判别粒度没有 GME 强

**Fused-Modal Data的消融**

<img width="564" alt="image" src="https://github.com/user-attachments/assets/eff25f6f-f992-4476-87ef-af710501a176" />

- 同时使用 FLUX.1 和 Google 检索的数据性能最好

**Training Scaling Law**

<img width="574" alt="image" src="https://github.com/user-attachments/assets/f0bdf556-99b9-4ffb-8428-0248640317c3" />

- 800W数据仍然没有饱和

**训练策略消融**

<img width="564" alt="image" src="https://github.com/user-attachments/assets/dd9a024d-08bf-4572-b39a-439a41793b6d" />

- 全参数微调的效果不如 LoRA，这点和 VLM2VEC 的结论一致，实在奇怪，理论上数据量也足够大了
